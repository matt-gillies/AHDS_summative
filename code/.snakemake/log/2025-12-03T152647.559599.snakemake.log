Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                   count    min threads    max threads
------------------  -------  -------------  -------------
all                       1              1              1
article_download          1              1              1
journal_formatting        1              1              1
list_download             1              1              1
total                     4              1              1

Select jobs to execute...

[Wed Dec  3 15:26:51 2025]
rule list_download:
    output: ../data/raw/IDs/IDs.xml
    jobid: 3
    reason: Code has changed since last execution
    resources: tmpdir=/tmp

[Wed Dec  3 15:26:52 2025]
Finished job 3.
1 of 4 steps (25%) done
Select jobs to execute...

[Wed Dec  3 15:26:52 2025]
rule article_download:
    input: ../data/raw/IDs/IDs.xml
    output: ../data/raw/Articles
    jobid: 2
    reason: Input files updated by another job: ../data/raw/IDs/IDs.xml
    resources: tmpdir=/tmp

[Wed Dec  3 15:27:11 2025]
Finished job 2.
2 of 4 steps (50%) done
Select jobs to execute...

[Wed Dec  3 15:27:12 2025]
rule journal_formatting:
    input: ../data/raw/Articles
    output: ../data/clean/journal_data_cleaned.tsv
    jobid: 1
    reason: Missing output files: ../data/clean/journal_data_cleaned.tsv; Input files updated by another job: ../data/raw/Articles
    resources: tmpdir=/tmp

[Wed Dec  3 15:27:14 2025]
Finished job 1.
3 of 4 steps (75%) done
Select jobs to execute...

[Wed Dec  3 15:27:14 2025]
localrule all:
    input: ../data/clean/journal_data_cleaned.tsv
    jobid: 0
    reason: Input files updated by another job: ../data/clean/journal_data_cleaned.tsv
    resources: tmpdir=/tmp

[Wed Dec  3 15:27:14 2025]
Finished job 0.
4 of 4 steps (100%) done
Complete log: .snakemake/log/2025-12-03T152647.559599.snakemake.log
