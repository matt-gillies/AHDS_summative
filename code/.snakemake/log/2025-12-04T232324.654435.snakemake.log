Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 3
Rules claiming more threads will be scaled down.
Job stats:
job                   count    min threads    max threads
------------------  -------  -------------  -------------
all                       1              1              1
journal_formatting        1              1              1
list_download             1              1              1
total                     3              1              1

Select jobs to execute...

[Thu Dec  4 23:23:27 2025]
checkpoint list_download:
    output: ../data/raw/PMIDs/ID_batches
    jobid: 2
    reason: Missing output files: ../data/raw/PMIDs/ID_batches
    resources: tmpdir=/tmp
DAG of jobs will be updated after completion.


        mkdir -p ../data/raw/PMIDs
        bash 1-ID_list_downl.sh True
        
[Thu Dec  4 23:23:28 2025]
Error in rule list_download:
    jobid: 2
    output: ../data/raw/PMIDs/ID_batches
    shell:
        
        mkdir -p ../data/raw/PMIDs
        bash 1-ID_list_downl.sh True
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job list_download since they might be corrupted:
../data/raw/PMIDs/ID_batches
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2025-12-04T232324.654435.snakemake.log
